TensorParallelism Backend

A minimal CUDA + NCCL backend simulating core components of tensor parallelism: device mesh, distributed tensor (DTensor), placements, and communication.

ðŸ“‚ Structure
backend/
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ mesh.hpp
â”‚   â”œâ”€â”€ cudafunctions.hpp
â”‚   â”œâ”€â”€ dtensor.hpp
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mesh.cu
â”‚   â”œâ”€â”€ cudafunctions.cpp
â”‚   â”œâ”€â”€ dtensor.cu
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_mesh.cpp
â”‚   â”œâ”€â”€ test_dtensor.cpp
â”œâ”€â”€ build.sh
â””â”€â”€ README.md


Build Instructions

chmod +x build.sh
./build.sh



Executables are created under build/:

./build/test_mesh
./build/test_dtensor



ðŸš€ Execution Flow

Mesh Initialization

Sets up GPU devices and NCCL communicators.

Builds logical mesh coordinates.

DTensor Creation

A global tensor (e.g., 8Ã—4) is divided across GPUs based on placements.

Example placement:

GPU 0 â†’ rows [0,3], columns [0,3]

GPU 1 â†’ rows [4,7], columns [0,3]

Printing and Cleanup

Shows mesh, placements, and local slices for each GPU.

Cleans up NCCL communicators.

ðŸ§© Example Output
[Mesh] Initializing mesh with 2 GPUs...
[Mesh] NCCL communicators initialized successfully.
[Mesh] Mesh shape set to [2]
[Mesh] GPU 0 logical coords: [0]
[Mesh] GPU 1 logical coords: [1]
[Mesh] Creating subgroup 'tensor' with devices: 0 1
[DTensor] Global shape: [8 x 4]
[DTensor Test] Printing GPU slices and placements:
[GPU 0] Placement: shard,replicate | Slices per dim: [0,3] [0,3]
[GPU 1] Placement: shard,replicate | Slices per dim: [4,7] [0,3]
[Mesh] Destroyed NCCL communicator.
